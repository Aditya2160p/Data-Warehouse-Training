{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86994bce-b190-4a8f-a079-1a4cb7a26786",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|Warehouse|NeedsCount|\n",
      "+---------+----------+\n",
      "+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Scenario 1: Inventory Alerting System\n",
    "# Tasks:\n",
    "# 1. Load the data using PySpark.\n",
    "# 2. Create a new column NeedsReorder = StockQty < ReorderLevel .\n",
    "# 3. Create a view of all items that need restocking.\n",
    "# 4. Highlight warehouses with more than 2 such items.\n",
    "from pyspark.sql.functions import *\n",
    "df = spark.read.csv(\"file:/Workspace/Shared/inventory_supply19.csv\",header=True,InferSchema=True)\n",
    "\n",
    "df = df.withColumn(\"NeedsReorder\", col(\"StockQty\") < col(\"ReorderLevel\"))\n",
    "\n",
    "df.filter(\"NeedsReorder\").createOrReplaceTempView(\"items_to_restock\")\n",
    "\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT Warehouse, COUNT(*) AS NeedsCount\n",
    "    FROM items_to_restock\n",
    "    GROUP BY Warehouse\n",
    "    HAVING COUNT(*) > 2\n",
    "    \"\"\"\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0148cf63-b080-4908-9675-809d151320e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "| Supplier|AvgPrice|\n",
      "+---------+--------+\n",
      "|TechWorld| 70000.0|\n",
      "|PrintFast|  8000.0|\n",
      "| FreezeIt| 25000.0|\n",
      "+---------+--------+\n",
      "\n",
      "+--------+------+---------+\n",
      "|Supplier|ItemID|UnitPrice|\n",
      "+--------+------+---------+\n",
      "+--------+------+---------+\n",
      "\n",
      "+---------+--------+\n",
      "| Supplier|GoodDeal|\n",
      "+---------+--------+\n",
      "|TechWorld|   false|\n",
      "|PrintFast|    true|\n",
      "| FreezeIt|   false|\n",
      "+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Scenario 2: Supplier Price Optimization\n",
    "# Tasks:\n",
    "# 1. Group items by Supplier and compute average price.\n",
    "# 2. Find which suppliers offer items below average price in their category.\n",
    "# 3. Tag suppliers with Good Deal if >50% of their items are below market average.\n",
    "spark.sql(\"\"\"\n",
    "SELECT Supplier, AVG(UnitPrice) AS AvgPrice\n",
    "FROM items_to_restock\n",
    "GROUP BY Supplier\n",
    "\"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT d.Supplier, d.ItemID, d.UnitPrice\n",
    "FROM items_to_restock d\n",
    "JOIN sup_avg_price s ON d.Supplier = s.Supplier\n",
    "WHERE d.UnitPrice < s.AvgPrice\n",
    "\"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT Supplier,\n",
    "  AVG(CASE WHEN UnitPrice < (SELECT AVG(UnitPrice) FROM items_to_restock WHERE Category = d.Category) THEN 1 ELSE 0 END) > 0.5 AS GoodDeal\n",
    "FROM items_to_restock d\n",
    "GROUP BY Supplier\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8d934b6-5f90-4a54-bcf0-5f7bbd40d581",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+-----------+----------+--------+------------+-------------+---------+---------+------------+---------------+\n",
      "|ItemID|    ItemName|   Category| Warehouse|StockQty|ReorderLevel|LastRestocked|UnitPrice| Supplier|NeedsReorder|TotalStockValue|\n",
      "+------+------------+-----------+----------+--------+------------+-------------+---------+---------+------------+---------------+\n",
      "|  I001|      LED TV|Electronics|WarehouseA|      50|          20|   2024-03-15|  30000.0|   AVTech|       false|      1500000.0|\n",
      "|  I002|      Laptop|Electronics|WarehouseB|      10|          15|   2024-04-01|  70000.0|TechWorld|        true|       700000.0|\n",
      "|  I003|Office Chair|  Furniture|WarehouseA|      40|          10|   2024-03-25|   6000.0|  ChairCo|       false|       240000.0|\n",
      "+------+------------+-----------+----------+--------+------------+-------------+---------+---------+------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Scenario 3: Cost Forecasting\n",
    "# Tasks:\n",
    "# 1. Calculate TotalStockValue = StockQty * UnitPrice .\n",
    "# 2. Identify top 3 highest-value items.\n",
    "# 3. Export the result as a Parquet file partitioned by Warehouse\n",
    "df = df.withColumn(\"TotalStockValue\", col(\"StockQty\") * col(\"UnitPrice\"))\n",
    "df.orderBy(col(\"TotalStockValue\").desc()).limit(3).show()\n",
    "\n",
    "df.write.mode(\"overwrite\").partitionBy(\"Warehouse\").parquet(\"file:/Workspace/Shared/top_stock_value\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fef6bf8-b846-4d01-9a31-e2b32d5cfdf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "| Warehouse|count|\n",
      "+----------+-----+\n",
      "|WarehouseA|    2|\n",
      "|WarehouseC|    1|\n",
      "|WarehouseB|    2|\n",
      "+----------+-----+\n",
      "\n",
      "+----------+-----------+-------------+\n",
      "| Warehouse|   Category|avg(StockQty)|\n",
      "+----------+-----------+-------------+\n",
      "|WarehouseB|Electronics|          6.5|\n",
      "|WarehouseA|  Furniture|         40.0|\n",
      "|WarehouseC| Appliances|          5.0|\n",
      "|WarehouseA|Electronics|         50.0|\n",
      "+----------+-----------+-------------+\n",
      "\n",
      "+----------+-------------+\n",
      "| Warehouse|sum(StockQty)|\n",
      "+----------+-------------+\n",
      "|WarehouseA|           90|\n",
      "|WarehouseC|            5|\n",
      "|WarehouseB|           13|\n",
      "+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Scenario 4: Warehouse Utilization\n",
    "# Tasks:\n",
    "# 1. Count items stored per warehouse.\n",
    "# 2. Average stock per category in each warehouse.\n",
    "# 3. Determine underutilized warehouses ( total stock < 100 ).\n",
    "\n",
    "df.groupBy(\"Warehouse\").count().show()\n",
    "\n",
    "df.groupBy(\"Warehouse\", \"Category\").avg(\"StockQty\").show()\n",
    "\n",
    "df.groupBy(\"Warehouse\").sum(\"StockQty\").filter(\"sum(StockQty) < 100\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6628336-e5d0-4071-b47a-635ed8681b31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+----------------+--------------------+---------+--------------------+----+------------------+--------------------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n",
      "|version|          timestamp|          userId|            userName|operation| operationParameters| job|          notebook|           clusterId|readVersion|   isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n",
      "+-------+-------------------+----------------+--------------------+---------+--------------------+----+------------------+--------------------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n",
      "|      3|2025-06-19 05:15:55|1679761755594499|azuser3546_mml.lo...| OPTIMIZE|{predicate -> [],...|NULL|{2977741827703191}|0612-091342-i15khidz|          1|SnapshotIsolation|        false|{numRemovedFiles ...|        NULL|Databricks-Runtim...|\n",
      "|      2|2025-06-19 05:15:53|1679761755594499|azuser3546_mml.lo...|   DELETE|{predicate -> [\"(...|NULL|{2977741827703191}|0612-091342-i15khidz|          1|WriteSerializable|        false|{numRemovedFiles ...|        NULL|Databricks-Runtim...|\n",
      "|      1|2025-06-19 05:15:52|1679761755594499|azuser3546_mml.lo...|   UPDATE|{predicate -> [\"(...|NULL|{2977741827703191}|0612-091342-i15khidz|          0|WriteSerializable|        false|{numRemovedFiles ...|        NULL|Databricks-Runtim...|\n",
      "|      0|2025-06-19 05:15:50|1679761755594499|azuser3546_mml.lo...|    WRITE|{mode -> Overwrit...|NULL|{2977741827703191}|0612-091342-i15khidz|       NULL|WriteSerializable|        false|{numFiles -> 1, n...|        NULL|Databricks-Runtim...|\n",
      "+-------+-------------------+----------------+--------------------+---------+--------------------+----+------------------+--------------------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n",
      "\n",
      "+------+------------+-----------+----------+--------+------------+-------------+---------+---------+------------+---------------+\n",
      "|ItemID|    ItemName|   Category| Warehouse|StockQty|ReorderLevel|LastRestocked|UnitPrice| Supplier|NeedsReorder|TotalStockValue|\n",
      "+------+------------+-----------+----------+--------+------------+-------------+---------+---------+------------+---------------+\n",
      "|  I001|      LED TV|Electronics|WarehouseA|      50|          20|   2024-03-15|  30000.0|   AVTech|       false|      1500000.0|\n",
      "|  I002|      Laptop|Electronics|WarehouseB|      10|          15|   2024-04-01|  70000.0|TechWorld|        true|       700000.0|\n",
      "|  I003|Office Chair|  Furniture|WarehouseA|      40|          10|   2024-03-25|   6000.0|  ChairCo|       false|       240000.0|\n",
      "|  I004|Refrigerator| Appliances|WarehouseC|       5|          10|   2024-02-20|  25000.0| FreezeIt|        true|       125000.0|\n",
      "|  I005|     Printer|Electronics|WarehouseB|       3|           5|   2024-03-30|   8000.0|PrintFast|        true|        24000.0|\n",
      "+------+------------+-----------+----------+--------+------------+-------------+---------+---------+------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Scenario 5: Delta Audit Trail\n",
    "# Tasks:\n",
    "# 1. Save as Delta table retail_inventory .\n",
    "# 2. Update stock of 'Laptop' to 20.\n",
    "# 3. Delete any item with StockQty = 0 .\n",
    "# 4. Run DESCRIBE HISTORY and query VERSION AS OF previous state.\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(\"file:/Worksapce/Shared/retail_inventory\")\n",
    "DeltaTable.forPath(spark, \"/delta/retail_inventory\")\n",
    "\n",
    "DeltaTable.forPath(spark, \"/delta/retail_inventory\").update(\n",
    "    condition=col(\"ItemName\") == \"Laptop\", set={\"StockQty\": \"20\"}\n",
    ")\n",
    "\n",
    "DeltaTable.forPath(spark, \"/delta/retail_inventory\").delete(\"StockQty = 0\")\n",
    "\n",
    "spark.sql(\"DESCRIBE HISTORY delta.`/delta/retail_inventory`\").show()\n",
    "spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(\"/delta/retail_inventory\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7c26b13-26d1-4848-a0eb-d28254acc664",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Scenario 6: Alerts from Restock Logs (Join Task)\n",
    "# restock_logs.csv :\n",
    "# ItemID,RestockDate,QuantityAdded\n",
    "# I002,2024-04-20,10\n",
    "# I005,2024-04-22,5\n",
    "# I001,2024-04-25,20\n",
    "# Tasks:\n",
    "# 1. Join with inventory table to update StockQty.\n",
    "# 2. Calculate new stock and flag RestockedRecently = true for updated items.\n",
    "# 3. Use MERGE INTO to update in Delta.\n",
    "\n",
    "logs = spark.read.csv(\"file:/Workspace/Shared/restock_logs19.csv\",header=True,InferSchema=True)\n",
    "\n",
    "joined = logs.join(df, \"ItemID\")\n",
    "updates = joined.withColumn(\"NewStock\", col(\"StockQty\") + col(\"QuantityAdded\")).withColumn(\"RestockedRecently\", lit(True))\n",
    "\n",
    "DeltaTable.forPath(spark, \"/delta/retail_inventory\").alias(\"inv\").merge(updates.alias(\"update\"),\"inv.ItemID = update.ItemID\").whenMatchedUpdate(set={\n",
    "  \"StockQty\": \"update.NewStock\",\n",
    "  \"LastRestocked\": \"update.RestockDate\"\n",
    "}).execute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea8ed774-f9cb-4276-b5a8-18a4da119089",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+--------+------------+---------------+\n",
      "|    ItemName|   Category|StockQty|NeedsReorder|TotalStockValue|\n",
      "+------------+-----------+--------+------------+---------------+\n",
      "|      LED TV|Electronics|      50|       false|        1500000|\n",
      "|      Laptop|Electronics|      10|        true|         700000|\n",
      "|Office Chair|  Furniture|      40|       false|         240000|\n",
      "|Refrigerator| Appliances|       5|        true|         125000|\n",
      "|     Printer|Electronics|       3|        true|          24000|\n",
      "+------------+-----------+--------+------------+---------------+\n",
      "\n",
      "+---------+--------+\n",
      "| Supplier|AvgPrice|\n",
      "+---------+--------+\n",
      "|  ChairCo|  6000.0|\n",
      "|PrintFast|  8000.0|\n",
      "| FreezeIt| 25000.0|\n",
      "|   AVTech| 30000.0|\n",
      "|TechWorld| 70000.0|\n",
      "+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Scenario 7: Report Generation with SQL Views\n",
    "# Tasks:\n",
    "# 1. Create SQL view inventory_summary with:\n",
    "# ItemName, Category, StockQty, NeedsReorder, TotalStockValue\n",
    "# 2. Create view supplier_leaderboard sorted by average price\n",
    "\n",
    "\n",
    "df.createOrReplaceTempView(\"inventory\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW inventory_summary AS\n",
    "    SELECT\n",
    "        ItemName,\n",
    "        Category,\n",
    "        StockQty,\n",
    "        CASE\n",
    "            WHEN StockQty < ReorderLevel THEN true\n",
    "            ELSE false\n",
    "        END AS NeedsReorder,\n",
    "        StockQty * UnitPrice AS TotalStockValue\n",
    "    FROM inventory\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW supplier_leaderboard AS\n",
    "    SELECT\n",
    "        Supplier,\n",
    "        ROUND(AVG(UnitPrice), 2) AS AvgPrice\n",
    "    FROM inventory\n",
    "    GROUP BY Supplier\n",
    "    ORDER BY AvgPrice\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"SELECT * FROM inventory_summary\").show()\n",
    "spark.sql(\"SELECT * FROM supplier_leaderboard\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45f283a9-8268-4a08-b8c9-9c86a5a5ea70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+-----------+----------+--------+------------+-------------+---------+---------+------------+---------------+-------------+\n",
      "|ItemID|    ItemName|   Category| Warehouse|StockQty|ReorderLevel|LastRestocked|UnitPrice| Supplier|NeedsReorder|TotalStockValue|StockCategory|\n",
      "+------+------------+-----------+----------+--------+------------+-------------+---------+---------+------------+---------------+-------------+\n",
      "|  I002|      Laptop|Electronics|WarehouseB|      10|          15|   2024-04-01|  70000.0|TechWorld|        true|       700000.0|     LowStock|\n",
      "|  I004|Refrigerator| Appliances|WarehouseC|       5|          10|   2024-02-20|  25000.0| FreezeIt|        true|       125000.0|     LowStock|\n",
      "|  I005|     Printer|Electronics|WarehouseB|       3|           5|   2024-03-30|   8000.0|PrintFast|        true|        24000.0|     LowStock|\n",
      "+------+------------+-----------+----------+--------+------------+-------------+---------+---------+------------+---------------+-------------+\n",
      "\n",
      "+------+------------+-----------+----------+--------+------------+-------------+---------+---------+------------+---------------+-------------+\n",
      "|ItemID|    ItemName|   Category| Warehouse|StockQty|ReorderLevel|LastRestocked|UnitPrice| Supplier|NeedsReorder|TotalStockValue|StockCategory|\n",
      "+------+------------+-----------+----------+--------+------------+-------------+---------+---------+------------+---------------+-------------+\n",
      "|  I002|      Laptop|Electronics|WarehouseB|      10|          15|   2024-04-01|  70000.0|TechWorld|        true|       700000.0|     LowStock|\n",
      "|  I004|Refrigerator| Appliances|WarehouseC|       5|          10|   2024-02-20|  25000.0| FreezeIt|        true|       125000.0|     LowStock|\n",
      "|  I005|     Printer|Electronics|WarehouseB|       3|           5|   2024-03-30|   8000.0|PrintFast|        true|        24000.0|     LowStock|\n",
      "+------+------------+-----------+----------+--------+------------+-------------+---------+---------+------------+---------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Scenario 8: Advanced Filtering\n",
    "# Tasks:\n",
    "# 1. Use when / otherwise to categorize items:\n",
    "# \"Overstocked\" (>2x ReorderLevel)\n",
    "# \"LowStock\"\n",
    "# 2. Use .filter() and .where() for the same and compare.\n",
    "df2 = df.withColumn(\"StockCategory\",when(col(\"StockQty\") > 2*col(\"ReorderLevel\"), \"Overstocked\").when(col(\"StockQty\") < col(\"ReorderLevel\"), \"LowStock\").otherwise(\"Normal\"))\n",
    "\n",
    "df2.filter(\"StockCategory = 'LowStock'\").show()\n",
    "df2.where(col(\"StockCategory\") == \"LowStock\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a452b4a-4583-4f3d-8a5f-5a3881665d7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Scenario 9: Feature Engineering\n",
    "# Tasks:\n",
    "# 1. Extract RestockMonth from LastRestocked .\n",
    "# 2. Create feature: StockAge = CURRENT_DATE - LastRestocked\n",
    "# 3. Bucket StockAge into: New, Moderate, Stale\n",
    "\n",
    "df = df.withColumn(\"RestockMonth\", month(\"LastRestocked\"))\n",
    "df = df.withColumn(\"StockAge\", datediff(current_date(), col(\"LastRestocked\")))\n",
    "df = df.withColumn(\"StockAgeType\",when(col(\"StockAge\") < 30, \"New\").when(col(\"StockAge\") < 90, \"Moderate\").otherwise(\"Stale\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f204c898-3e01-44ac-921b-318858bc005e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Scenario 10: Export Options\n",
    "# Tasks:\n",
    "# 1. Write full DataFrame to:\n",
    "# CSV for analysts\n",
    "# JSON for integration\n",
    "# Delta for pipelines\n",
    "# 2. Save with meaningful file and partition names like\n",
    "# /export/inventory/stale_items/\n",
    "df.write.mode(\"overwrite\").csv(\"file:/Workspace/Shared/inventory_full\", header=True)\n",
    "df.write.mode(\"overwrite\").json(\"file:/Workspace/Shared/inventory_full_json\")\n",
    "df.write.mode(\"overwrite\").format(\"delta\").save(\"file:/Workspace/Shared/inventory_delta\")\n",
    "\n",
    "df.write.mode(\"overwrite\").format(\"delta\").partitionBy(\"Category\").save(\"file:/Workspace/Shared/inventory_delta2\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "june19set3",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
