{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74370c3a-6742-40e8-8c5f-97b2a0fd435a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+--------+-----------------+------------+----------+--------------+---------------+-------+--------------+\n",
      "|EnrollID|UserID|CourseID|       CourseName|    Category|EnrollDate|CompletionDate|ProgressPercent|Ratings|DaysToComplete|\n",
      "+--------+------+--------+-----------------+------------+----------+--------------+---------------+-------+--------------+\n",
      "|    E001|  U001|    C001|    Python Basics| Programming|2024-04-01|    2024-04-10|            100|      4|             9|\n",
      "|    E002|  U002|    C002|Excel for Finance|Productivity|2024-04-02|          NULL|             45|   NULL|          NULL|\n",
      "|    E003|  U001|    C003|  ML with PySpark|Data Science|2024-04-03|          NULL|             30|   NULL|          NULL|\n",
      "|    E004|  U003|    C001|    Python Basics| Programming|2024-04-04|    2024-04-20|            100|      5|            16|\n",
      "|    E005|  U004|    C004|Digital Marketing|   Marketing|2024-04-05|    2024-04-16|            100|      4|            11|\n",
      "+--------+------+--------+-----------------+------------+----------+--------------+---------------+-------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1\n",
    "# Ingestion & Time Fields\n",
    "# Load into PySpark with inferred schema\n",
    "# Convert EnrollDate and CompletionDate to date type\n",
    "# Add DaysToComplete column if completed\n",
    "df = spark.read.csv(\"file:/Workspace/Shared/course_enrollments19.csv\",header=True,inferSchema=True)\n",
    "\n",
    "df = df.withColumn(\"EnrollDate\", to_date(\"EnrollDate\")).withColumn(\"CompletionDate\", to_date(\"CompletionDate\")).withColumn(\"DaysToComplete\", when(col(\"CompletionDate\").isNotNull(),datediff(col(\"CompletionDate\"), col(\"EnrollDate\"))))\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60ee28b1-7ec7-4834-8f58-3e192a93c077",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+-----------+-----------+\n",
      "|UserID|CoursesEnrolled|AvgProgress|IsCompleted|\n",
      "+------+---------------+-----------+-----------+\n",
      "|  U004|              1|      100.0|       true|\n",
      "|  U002|              1|       45.0|      false|\n",
      "|  U003|              1|      100.0|       true|\n",
      "|  U001|              2|       65.0|      false|\n",
      "+------+---------------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2\n",
    "# User Learning Path Progress\n",
    "# Group by UserID : count of courses enrolled\n",
    "# Avg progress % across all enrollments\n",
    "# Flag IsCompleted = ProgressPercent = 100\n",
    "\n",
    "user_progress = df.groupBy(\"UserID\").agg(count(\"*\").alias(\"CoursesEnrolled\"),avg(\"ProgressPercent\").alias(\"AvgProgress\")).withColumn(\"IsCompleted\", col(\"AvgProgress\") == 100)\n",
    "\n",
    "user_progress.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1de283e-0d09-4744-aade-7dacedeb8e6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+------------+---------------+\n",
      "|EnrollID|ProgressPercent|RatingFilled|EngagementScore|\n",
      "+--------+---------------+------------+---------------+\n",
      "|    E001|            100|           4|            400|\n",
      "|    E002|             45|        NULL|           NULL|\n",
      "|    E003|             30|        NULL|           NULL|\n",
      "|    E004|            100|           5|            500|\n",
      "|    E005|            100|           4|            400|\n",
      "+--------+---------------+------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 3\n",
    "# Engagement Scoring\n",
    "# Create a score: ProgressPercent * Rating (if not null)\n",
    "# Replace null Rating with 0 before computing\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "df = df.withColumn(\"RatingFilled\", coalesce(col(\"Ratings\"), col(\"Ratings\")*0)).withColumn(\"EngagementScore\", col(\"ProgressPercent\") * col(\"RatingFilled\"))\n",
    "\n",
    "df.select(\"EnrollID\", \"ProgressPercent\", \"RatingFilled\", \"EngagementScore\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3be0de8f-ae19-4109-9c30-e7cfcd349e70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+--------+-----------------+------------+----------+--------------+---------------+-------+--------------+------------+---------------+\n",
      "|EnrollID|UserID|CourseID|       CourseName|    Category|EnrollDate|CompletionDate|ProgressPercent|Ratings|DaysToComplete|RatingFilled|EngagementScore|\n",
      "+--------+------+--------+-----------------+------------+----------+--------------+---------------+-------+--------------+------------+---------------+\n",
      "|    E002|  U002|    C002|Excel for Finance|Productivity|2024-04-02|          NULL|             45|   NULL|          NULL|        NULL|           NULL|\n",
      "|    E003|  U001|    C003|  ML with PySpark|Data Science|2024-04-03|          NULL|             30|   NULL|          NULL|        NULL|           NULL|\n",
      "+--------+------+--------+-----------------+------------+----------+--------------+---------------+-------+--------------+------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 4\n",
    "# Identify Drop-offs\n",
    "# Filter all records with ProgressPercent < 50 and CompletionDate is null\n",
    "# Create a view called Dropouts\n",
    "df.createOrReplaceTempView(\"enrollments\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TEMP VIEW Dropouts AS\n",
    "SELECT *\n",
    "FROM enrollments\n",
    "WHERE ProgressPercent < 50\n",
    "  AND CompletionDate IS NULL\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"SELECT * FROM Dropouts\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1bf64c0-a9c7-45db-b3b2-dcf11b2b5809",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+-----------+\n",
      "|   Instructor|AvgProgress|EnrollCount|\n",
      "+-------------+-----------+-----------+\n",
      "|  Zoya Sheikh|      100.0|          1|\n",
      "|   Sana Gupta|       45.0|          1|\n",
      "| Ibrahim Khan|       30.0|          1|\n",
      "|Abdullah Khan|      100.0|          2|\n",
      "+-------------+-----------+-----------+\n",
      "\n",
      "+--------+-----------+\n",
      "|CourseID|TotalEnroll|\n",
      "+--------+-----------+\n",
      "|    C001|          2|\n",
      "+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 5\n",
    "# Joins with Metadata\n",
    "# Create course_catalog.csv :\n",
    "# CourseID,Instructor,DurationHours,Level\n",
    "# C001,Abdullah Khan,8,Beginner\n",
    "# C002,Sana Gupta,5,Beginner\n",
    "# C003,Ibrahim Khan,10,Intermediate\n",
    "# C004,Zoya Sheikh,6,Beginner\n",
    "# Join to find average progress per instructor\n",
    "# Show who teaches the most enrolled course\n",
    "catalog = spark.read.csv(\"file:/Workspace/Shared/course_catalog19.csv\",header=True)\n",
    "catalog.createOrReplaceTempView(\"catalog\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT c.Instructor,\n",
    "       ROUND(AVG(e.ProgressPercent),1) AS AvgProgress,\n",
    "       COUNT(*) AS EnrollCount\n",
    "FROM enrollments e\n",
    "JOIN catalog c ON e.CourseID = c.CourseID\n",
    "GROUP BY c.Instructor\n",
    "\"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT c.CourseID, COUNT(*) AS TotalEnroll\n",
    "FROM enrollments e\n",
    "JOIN catalog c ON e.CourseID = c.CourseID\n",
    "GROUP BY c.CourseID\n",
    "ORDER BY TotalEnroll DESC\n",
    "LIMIT 1\n",
    "\"\"\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dda2df01-e015-41c4-bc7f-20f8d41d1f7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+----------------+--------------------+---------+--------------------+----+------------------+--------------------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n",
      "|version|           timestamp|          userId|            userName|operation| operationParameters| job|          notebook|           clusterId|readVersion|   isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n",
      "+-------+--------------------+----------------+--------------------+---------+--------------------+----+------------------+--------------------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n",
      "|      9|2025-06-19 05:50:...|1679761755594499|azuser3546_mml.lo...| OPTIMIZE|{predicate -> [],...|NULL|{2977741827703190}|0612-091342-i15khidz|          7|SnapshotIsolation|        false|{numRemovedFiles ...|        NULL|Databricks-Runtim...|\n",
      "|      8|2025-06-19 05:50:...|1679761755594499|azuser3546_mml.lo...|   DELETE|{predicate -> [\"(...|NULL|{2977741827703190}|0612-091342-i15khidz|          7|WriteSerializable|        false|{numRemovedFiles ...|        NULL|Databricks-Runtim...|\n",
      "|      7|2025-06-19 05:50:...|1679761755594499|azuser3546_mml.lo...|   UPDATE|{predicate -> [\"(...|NULL|{2977741827703190}|0612-091342-i15khidz|          6|WriteSerializable|        false|{numRemovedFiles ...|        NULL|Databricks-Runtim...|\n",
      "|      6|2025-06-19 05:50:...|1679761755594499|azuser3546_mml.lo...|    WRITE|{mode -> Overwrit...|NULL|{2977741827703190}|0612-091342-i15khidz|          5|WriteSerializable|        false|{numFiles -> 1, n...|        NULL|Databricks-Runtim...|\n",
      "|      5|2025-06-19 05:49:...|1679761755594499|azuser3546_mml.lo...| OPTIMIZE|{predicate -> [],...|NULL|{2977741827703190}|0612-091342-i15khidz|          3|SnapshotIsolation|        false|{numRemovedFiles ...|        NULL|Databricks-Runtim...|\n",
      "|      4|2025-06-19 05:49:...|1679761755594499|azuser3546_mml.lo...|   DELETE|{predicate -> [\"(...|NULL|{2977741827703190}|0612-091342-i15khidz|          3|WriteSerializable|        false|{numRemovedFiles ...|        NULL|Databricks-Runtim...|\n",
      "|      3|2025-06-19 05:49:...|1679761755594499|azuser3546_mml.lo...|   UPDATE|{predicate -> [\"(...|NULL|{2977741827703190}|0612-091342-i15khidz|          2|WriteSerializable|        false|{numRemovedFiles ...|        NULL|Databricks-Runtim...|\n",
      "|      2|2025-06-19 05:49:...|1679761755594499|azuser3546_mml.lo...|    WRITE|{mode -> Overwrit...|NULL|{2977741827703190}|0612-091342-i15khidz|          1|WriteSerializable|        false|{numFiles -> 1, n...|        NULL|Databricks-Runtim...|\n",
      "|      1|2025-06-19 05:49:...|1679761755594499|azuser3546_mml.lo...|    WRITE|{mode -> Overwrit...|NULL|{2977741827703190}|0612-091342-i15khidz|          0|WriteSerializable|        false|{numFiles -> 1, n...|        NULL|Databricks-Runtim...|\n",
      "|      0|2025-06-19 05:49:...|1679761755594499|azuser3546_mml.lo...|    WRITE|{mode -> Overwrit...|NULL|{2977741827703190}|0612-091342-i15khidz|       NULL|WriteSerializable|        false|{numFiles -> 1, n...|        NULL|Databricks-Runtim...|\n",
      "+-------+--------------------+----------------+--------------------+---------+--------------------+----+------------------+--------------------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n",
      "\n",
      "+-----------------+-----------+----+\n",
      "|       CourseName|EnrollCount|Rank|\n",
      "+-----------------+-----------+----+\n",
      "|    Python Basics|          2|   1|\n",
      "|Digital Marketing|          1|   2|\n",
      "|Excel for Finance|          1|   2|\n",
      "|  ML with PySpark|          1|   2|\n",
      "+-----------------+-----------+----+\n",
      "\n",
      "+--------+------+--------+-----------------+------------+----------+--------------+---------------+-------+--------------+------------+---------------+---------------+\n",
      "|EnrollID|UserID|CourseID|       CourseName|    Category|EnrollDate|CompletionDate|ProgressPercent|Ratings|DaysToComplete|RatingFilled|EngagementScore|     NextCourse|\n",
      "+--------+------+--------+-----------------+------------+----------+--------------+---------------+-------+--------------+------------+---------------+---------------+\n",
      "|    E001|  U001|    C001|    Python Basics| Programming|2024-04-01|    2024-04-10|            100|      4|             9|           4|            400|ML with PySpark|\n",
      "|    E003|  U001|    C003|  ML with PySpark|Data Science|2024-04-03|          NULL|             30|   NULL|          NULL|        NULL|           NULL|           NULL|\n",
      "|    E002|  U002|    C002|Excel for Finance|Productivity|2024-04-02|          NULL|             45|   NULL|          NULL|        NULL|           NULL|           NULL|\n",
      "|    E004|  U003|    C001|    Python Basics| Programming|2024-04-04|    2024-04-20|            100|      5|            16|           5|            500|           NULL|\n",
      "|    E005|  U004|    C004|Digital Marketing|   Marketing|2024-04-05|    2024-04-16|            100|      4|            11|           4|            400|           NULL|\n",
      "+--------+------+--------+-----------------+------------+----------+--------------+---------------+-------+--------------+------------+---------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 6\n",
    "# Delta Lake Practice\n",
    "# Save as Delta Table enrollments_delta\n",
    "# Apply:\n",
    "# Update: Set all ratings to 5 where Course = 'Python Basics'\n",
    "# Delete: All rows where ProgressPercent = 0\n",
    "# Show DESCRIBE HISTORY\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(\"file:/Workspace/Shared/enrollments\")\n",
    "delta = DeltaTable.forPath(spark, \"file:/Workspace/Shared/enrollments\")\n",
    "\n",
    "delta.update(\n",
    "    condition=\"CourseName = 'Python Basics'\",\n",
    "    set={\"Ratings\": \"5\"}\n",
    ")\n",
    "\n",
    "delta.delete(\"ProgressPercent = 0\")\n",
    "\n",
    "delta.history().show()\n",
    "\n",
    "# 7\n",
    "# Window Functions\n",
    "# Use dense_rank() to rank courses by number of enrollments\n",
    "# lead() to find next course by each user (sorted by EnrollDate)\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "window1 = Window.orderBy(col(\"EnrollCount\").desc())\n",
    "enrollment_counts = spark.sql(\"\"\"\n",
    "SELECT CourseName, COUNT(*) AS EnrollCount\n",
    "FROM enrollments\n",
    "GROUP BY CourseName\n",
    "\"\"\")\n",
    "enrollment_counts.withColumn(\"Rank\", dense_rank().over(window1)).show()\n",
    "\n",
    "window2 = Window.partitionBy(\"UserID\").orderBy(\"EnrollDate\")\n",
    "df.withColumn(\"NextCourse\", lead(\"CourseName\").over(window2)).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f4cb087-9b75-4b63-aa6e-6ddeacf36cf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+--------+-----------------+------------+----------+--------------+---------------+-------+--------------+------------+---------------+\n",
      "|EnrollID|UserID|CourseID|       CourseName|    Category|EnrollDate|CompletionDate|ProgressPercent|Ratings|DaysToComplete|RatingFilled|EngagementScore|\n",
      "+--------+------+--------+-----------------+------------+----------+--------------+---------------+-------+--------------+------------+---------------+\n",
      "|    E001|  U001|    C001|    Python Basics| Programming|2024-04-01|    2024-04-10|            100|      4|             9|           4|            400|\n",
      "|    E002|  U002|    C002|Excel for Finance|Productivity|2024-04-02|          NULL|             45|   NULL|          NULL|        NULL|           NULL|\n",
      "|    E003|  U001|    C003|  ML with PySpark|Data Science|2024-04-03|          NULL|             30|   NULL|          NULL|        NULL|           NULL|\n",
      "|    E004|  U003|    C001|    Python Basics| Programming|2024-04-04|    2024-04-20|            100|      5|            16|           5|            500|\n",
      "|    E005|  U004|    C004|Digital Marketing|   Marketing|2024-04-05|    2024-04-16|            100|      4|            11|           4|            400|\n",
      "+--------+------+--------+-----------------+------------+----------+--------------+---------------+-------+--------------+------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import round\n",
    "# 8\n",
    "# SQL Logic for Dashboard Views\n",
    "# Create views:\n",
    "# daily_enrollments\n",
    "# category_performance (avg rating by category)\n",
    "# top_3_courses\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TEMP VIEW daily_enrollments AS\n",
    "SELECT EnrollDate, COUNT(*) AS TotalEnrolled\n",
    "FROM enrollments\n",
    "GROUP BY EnrollDate\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TEMP VIEW category_performance AS\n",
    "SELECT Category, ROUND(AVG(Ratings),2) AS AvgRating\n",
    "FROM enrollments\n",
    "GROUP BY Category\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TEMP VIEW top_3_courses AS\n",
    "SELECT CourseName, COUNT(*) AS EnrollCount\n",
    "FROM enrollments\n",
    "GROUP BY CourseName\n",
    "ORDER BY EnrollCount DESC\n",
    "LIMIT 3\n",
    "\"\"\")\n",
    "\n",
    "# 9\n",
    "# Time Travel\n",
    "# View previous version before update/delete\n",
    "# Use VERSION AS OF and TIMESTAMP AS OF\n",
    "# Export Reporting\n",
    "# Write to JSON, partitioned by Category\n",
    "# Create summary DataFrame:\n",
    "# CourseName, TotalEnrollments, AvgRating, AvgProgress\n",
    "# Save as Parquet\n",
    "\n",
    "spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(\"file:/Workspace/Shared/enrollments\").show()\n",
    "\n",
    "final_df = spark.table(\"enrollments\")\n",
    "\n",
    "final_df.write.mode(\"overwrite\").partitionBy(\"Category\").parquet(\"file:/Workspace/Shared/enrollments_by_category\")\n",
    "summary_df = final_df.groupBy(\"CourseName\").agg(\n",
    "    count(\"*\").alias(\"TotalEnrollments\"),\n",
    "    round(avg(\"RatingFilled\"),2).alias(\"AvgRating\"),\n",
    "    round(avg(\"ProgressPercent\"),2).alias(\"AvgProgress\")\n",
    ")\n",
    "summary_df.write.mode(\"overwrite\").parquet(\"file:/Workspace/Shared/course_summary_parquet\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "june19set2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
