{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db3d61c5-08bc-4752-8d8f-6dbdf39223cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=2209457512301869#setting/sparkui/0611-043336-syhwrm2t/driver-234706838216879812\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*, 4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7013d2f71450>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark=SparkSession.builder.appName(\"EmployeeData\").getOrCreate()\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e7ec357-4408-4bf8-a19f-31e4d7efced6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "(\"Ananya\", \"HR\", 52000),\n",
    "(\"Rahul\", \"Engineering\", 65000),\n",
    "(\"Priya\", \"Engineering\", 60000),\n",
    "(\"Zoya\", \"Marketing\", 48000),\n",
    "(\"Karan\", \"HR\", 53000),\n",
    "(\"Naveen\", \"Engineering\", 70000),\n",
    "(\"Fatima\", \"Marketing\", 45000)\n",
    "]\n",
    "columns = [\"Name\", \"Department\", \"Salary\"]\n",
    "empdf = spark.createDataFrame(data, columns)\n",
    "performance = [\n",
    "    (\"Ananya\", 2023, 4.5),\n",
    "    (\"Rahul\", 2023, 4.9),\n",
    "    (\"Priya\", 2023, 4.3),\n",
    "    (\"Zoya\", 2023, 3.8),\n",
    "    (\"Karan\", 2023, 4.1),\n",
    "    (\"Naveen\", 2023, 4.7),\n",
    "    (\"Fatima\", 2023, 3.9)\n",
    "]\n",
    "columns_perf = [\"Name\", \"Year\", \"Rating\"]\n",
    "perfdf = spark.createDataFrame(performance, columns_perf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4f5092a-064d-421e-b43f-869d15f8c1fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n| Department|avg(Salary)|\n+-----------+-----------+\n|         HR|    52500.0|\n|Engineering|    65000.0|\n|  Marketing|    46500.0|\n+-----------+-----------+\n\n+-----------+-----+\n| Department|count|\n+-----------+-----+\n|         HR|    2|\n|Engineering|    3|\n|  Marketing|    2|\n+-----------+-----+\n\n+---------+---------+\n|MaxSalary|MinSalary|\n+---------+---------+\n|    70000|    60000|\n+---------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# GroupBy and Aggregations\n",
    "# 1. Get the average salary by department.\n",
    "# 2. Count of employees per department.\n",
    "# 3. Maximum and minimum salary in Engineering.\n",
    "from pyspark.sql.functions import col, max, min \n",
    "empdf.groupBy(\"Department\").avg(\"Salary\").show()\n",
    "empdf.groupBy(\"Department\").count().show()\n",
    "empdf.filter(col(\"Department\") == \"Engineering\").agg(max(\"Salary\").alias(\"MaxSalary\"), min(\"Salary\").alias(\"MinSalary\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bd3bf58-8b75-4900-9b2d-5d1e3310da21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+----+------+\n|  Name| Department|Salary|Year|Rating|\n+------+-----------+------+----+------+\n|Ananya|         HR| 52000|2023|   4.5|\n|Fatima|  Marketing| 45000|2023|   3.9|\n| Karan|         HR| 53000|2023|   4.1|\n|Naveen|Engineering| 70000|2023|   4.7|\n| Priya|Engineering| 60000|2023|   4.3|\n| Rahul|Engineering| 65000|2023|   4.9|\n|  Zoya|  Marketing| 48000|2023|   3.8|\n+------+-----------+------+----+------+\n\n+------+------+------+\n|  Name|Salary|Rating|\n+------+------+------+\n|Ananya| 52000|   4.5|\n|Fatima| 45000|   3.9|\n| Karan| 53000|   4.1|\n|Naveen| 70000|   4.7|\n| Priya| 60000|   4.3|\n| Rahul| 65000|   4.9|\n|  Zoya| 48000|   3.8|\n+------+------+------+\n\n+------+-----------+------+----+------+\n|  Name| Department|Salary|Year|Rating|\n+------+-----------+------+----+------+\n|Naveen|Engineering| 70000|2023|   4.7|\n| Rahul|Engineering| 65000|2023|   4.9|\n+------+-----------+------+----+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Join and Combine Data\n",
    "# 4. Perform an inner join between employee_data and performance_data on Name .\n",
    "# 5. Show each employeeâ€™s salary and performance rating.\n",
    "# 6. Filter employees with rating > 4.5 and salary > 60000.\n",
    "from pyspark.sql.functions import col\n",
    "joined = empdf.join(perfdf, on=\"Name\", how=\"inner\")\n",
    "joined.show()\n",
    "joined.select(\"Name\", \"Salary\", \"Rating\").show()\n",
    "joined.filter((col(\"Rating\") > 4.5) & (col(\"Salary\") > 60000)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9282bba-c835-43a5-9c84-730f1709a808",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+----+\n|  Name| Department|Salary|Rank|\n+------+-----------+------+----+\n|Naveen|Engineering| 70000|   1|\n| Rahul|Engineering| 65000|   2|\n| Priya|Engineering| 60000|   3|\n| Karan|         HR| 53000|   1|\n|Ananya|         HR| 52000|   2|\n|  Zoya|  Marketing| 48000|   1|\n|Fatima|  Marketing| 45000|   2|\n+------+-----------+------+----+\n\n+------+-----------+------+----------------+\n|  Name| Department|Salary|CumulativeSalary|\n+------+-----------+------+----------------+\n| Priya|Engineering| 60000|           60000|\n| Rahul|Engineering| 65000|          125000|\n|Naveen|Engineering| 70000|          195000|\n|Ananya|         HR| 52000|           52000|\n| Karan|         HR| 53000|          105000|\n|Fatima|  Marketing| 45000|           45000|\n|  Zoya|  Marketing| 48000|           93000|\n+------+-----------+------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Window & Rank (Bonus Challenge)\n",
    "# 7. Rank employees by salary department-wise.\n",
    "# 8. Calculate cumulative salary in each department.\n",
    "from pyspark.sql.functions import rank,_sum\n",
    "from pyspark.sql.window import Window\n",
    "rank = Window.partitionBy(\"Department\").orderBy(col(\"Salary\").desc())\n",
    "empdf.withColumn(\"Rank\", rank().over(rank)).show()\n",
    "cumulative = Window.partitionBy(\"Department\").orderBy(\"Salary\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "empdf.withColumn(\"CumulativeSalary\", _sum(\"Salary\").over(cumulative)).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcf74f50-a97f-4907-8e61-3e86edd10fe5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+----------+\n|  Name| Department|Salary|  JoinDate|\n+------+-----------+------+----------+\n|Ananya|         HR| 52000|2023-10-19|\n| Rahul|Engineering| 65000|2023-01-02|\n| Priya|Engineering| 60000|2020-10-07|\n|  Zoya|  Marketing| 48000|2020-04-07|\n| Karan|         HR| 53000|2022-09-30|\n|Naveen|Engineering| 70000|2020-07-13|\n|Fatima|  Marketing| 45000|2021-01-23|\n+------+-----------+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Date Operations\n",
    "# 9. Add a new column JoinDate with random dates between 2020 and 2023.\n",
    "# 10. Add column YearsWithCompany using current_date() and datediff() .\n",
    "from pyspark.sql.functions import current_date, datediff, monotonically_increasing_id\n",
    "import random\n",
    "from datetime import date, timedelta\n",
    "\n",
    "def random_date():\n",
    "    start_date = date(2020, 1, 1)\n",
    "    end_date = date(2023, 12, 31)\n",
    "    delta = end_date - start_date\n",
    "    random_days = random.randint(0, delta.days)\n",
    "    return start_date + timedelta(days=random_days)\n",
    " \n",
    " \n",
    "join_dates = [(random_date(),) for _ in range(empdf.count())]\n",
    "date_df = spark.createDataFrame(join_dates, [\"JoinDate\"]).withColumn(\"id\", monotonically_increasing_id())\n",
    "df_emp_id = empdf.withColumn(\"id\", monotonically_increasing_id())\n",
    "df_emp_with_date = df_emp_id.join(date_df, on=\"id\").drop(\"id\")\n",
    "\n",
    "df_emp_with_date.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7dcf7612-c1b3-40d7-a420-b0a93d9f2de4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Writing to Files\n",
    "# 11. Write the full employee DataFrame to CSV with headers.\n",
    "# 12. Save the joined DataFrame to a Parquet file.\n",
    "\n",
    "empdf.write.option(\"header\", True).csv(\"output/employee_data_csv\", mode=\"overwrite\")\n",
    "joined.write.parquet(\"output/employee_performance_parquet\", mode=\"overwrite\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Aggr_and_Grouping",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}