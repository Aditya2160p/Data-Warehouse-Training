{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76cca71c-5c6a-4663-a74a-1835c1dce21f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "#  PySpark + Delta\n",
    "#  1. Ingest all 3 CSVs as Delta Tables.\n",
    "\n",
    "ordersdf = spark.read.option(\"header\", True).csv(\"file:/Workspace/Shared/orders.csv\")\n",
    "customersdf = spark.read.option(\"header\", True).csv(\"file:/Workspace/Shared/customers.csv\")\n",
    "productsdf = spark.read.option(\"header\", True).csv(\"file:/Workspace/Shared/products.csv\")\n",
    "\n",
    "ordersdf.write.mode(\"overwrite\").format(\"delta\").save(\"file:/Workspace/Shared/orders\")\n",
    "customersdf.write.mode(\"overwrite\").format(\"delta\").save(\"file:/Workspace/Shared/customers\")\n",
    "productsdf.write.mode(\"overwrite\").format(\"delta\").save(\"file:/Workspace/Shared/products\")\n",
    "\n",
    "#  2. Write SQL to get the total revenue per Product.\n",
    "orddelta = spark.read.format(\"delta\").load(\"file:/Workspace/Shared/orders\")\n",
    "revenue_per_product = orddelta.filter(col(\"Status\") == \"Delivered\").withColumn(\"Revenue\", col(\"Quantity\") * col(\"Price\")).groupBy(\"ProductID\").agg(_sum(\"Revenue\").alias(\"TotalRevenue\"))\n",
    "\n",
    "revenue_per_product.show()\n",
    "\n",
    "#  3. Join Orders + Customers to find revenue by Region.\n",
    "\n",
    "customers_delta = spark.read.format(\"delta\").load(\"file:/Workspace/Shared/customers\")\n",
    "orders_with_customers = orders_delta.join(customers_delta, \"CustomerID\")\n",
    "\n",
    "revenue_by_region = orders_with_customers.filter(col(\"Status\") == \"Delivered\").withColumn(\"Revenue\", col(\"Quantity\") * col(\"Price\")).groupBy(\"Region\").agg(_sum(\"Revenue\").alias(\"TotalRevenue\"))\n",
    "\n",
    "revenue_by_region.show()\n",
    "\n",
    "#  4. Update the Status of Pending orders to 'Cancelled'.\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "delta_orders = DeltaTable.forPath(spark, \"file:/Workspace/Shared/orders\")\n",
    "delta_orders.update(\n",
    "    condition = col(\"Status\") == \"Pending\",\n",
    "    set = {\"Status\": \"Cancelled\"}\n",
    ")\n",
    "\n",
    "#  5. Merge a new return record into Orders.\n",
    "from pyspark.sql import Row\n",
    "new_return = [Row(OrderID=\"3006\", CustomerID=\"C002\", ProductID=\"P1002\", Quantity=1, Price=50000,\n",
    "                  OrderDate=\"2024-05-06\", Status=\"Returned\")]\n",
    "new_return_df = spark.createDataFrame(new_return)\n",
    "\n",
    "delta_orders.alias(\"orders\").merge(new_return_df.alias(\"new\"),\"orders.OrderID = new.OrderID\").whenNotMatchedInsertAll().execute()\n",
    "\n",
    "#  DLT Pipeline\n",
    "#  6. Create raw → cleaned → aggregated tables:\n",
    "#  Clean: Remove rows with NULLs\n",
    "#  Aggregated: Total revenue per Category\n",
    "cleaned_orders = orders_delta.dropna()\n",
    "\n",
    "\n",
    "products_delta = spark.read.format(\"delta\").load(\"file:/Workspace/Shared/products\")\n",
    "joined = cleaned_orders.filter(col(\"Status\") == \"Delivered\").withColumn(\"Revenue\", col(\"Quantity\") * col(\"Price\")).join(products_delta, \"ProductID\")\n",
    "\n",
    "revenue_per_category = joined.groupBy(\"Category\").agg(_sum(\"Revenue\").alias(\"TotalRevenue\"))\n",
    "revenue_per_category.show()\n",
    "\n",
    "#  Time Travel\n",
    "#  7. View data before the Status update.\n",
    "#  8. Restore to an older version of the orders table.\n",
    "#  Vacuum + Retention\n",
    "\n",
    "spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(\"file:/Workspace/Shared/orders\").show()\n",
    "\n",
    "# 8. Restore Old Version\n",
    "delta_orders.restoreToVersion(0) \n",
    "\n",
    "# 9. Run \n",
    "# VACUUM after changing default retention.\n",
    "#  Expectations\n",
    "\n",
    "spark.sql(\"SET spark.databricks.delta.retentionDurationCheck.enabled = false\")\n",
    "delta_orders.vacuum(0)\n",
    "\n",
    "#  10. \n",
    "# Quantity > 0 , \n",
    "# Price > 0 , \n",
    "# Bonus\n",
    "\n",
    "quality_checked = orders_delta.filter((col(\"Quantity\") > 0) & (col(\"Price\") > 0) & (col(\"OrderDate\").isNotNull()))\n",
    "\n",
    "#  11. Use \n",
    "# OrderDate is not null\n",
    "#  when-otherwise to create a new column: \n",
    "# 'Returned\n",
    "final_orders = quality_checked.withColumn(\"OrderType\",when(col(\"Status\") == \"Returned\", \"Return\").otherwise(\"Regular\"))\n",
    "\n",
    "final_orders.select(\"OrderID\", \"Status\", \"OrderType\").show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "june 17 a1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}