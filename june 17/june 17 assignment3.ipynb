{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3224263b-b3de-4c1f-a1da-f8e3b8cfeb13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=430135978535196#setting/sparkui/0612-091342-i15khidz/driver-6833277981965674073\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*, 4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7b2c85508e50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb963cf1-0cc5-42e8-b098-dab47c7c91a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------+-------+---------+----------+---------+-------+\n|EmployeeID| Name|Department|Project|WorkHours|  WorkDate| Location|  Mode |\n+----------+-----+----------+-------+---------+----------+---------+-------+\n|      E101|Anita|        IT|  Alpha|        8|2024-05-01|Bangalore|Remote |\n|      E102|  Raj|        HR|   Beta|        7|2024-05-01|   Mumbai|Onsite |\n|      E103| John|   Finance|  Alpha|        5|2024-05-02|    Delhi|Remote |\n|      E101|Anita|        IT|  Alpha|        9|2024-05-03|Bangalore|Remote |\n|      E104|Meena|        IT|  Gamma|        6|2024-05-03|Hyderabad|Onsite |\n|      E102|  Raj|        HR|   Beta|        8|2024-05-04|   Mumbai| Remote|\n+----------+-----+----------+-------+---------+----------+---------+-------+\n\nroot\n |-- EmployeeID: string (nullable = true)\n |-- Name: string (nullable = true)\n |-- Department: string (nullable = true)\n |-- Project: string (nullable = true)\n |-- WorkHours: integer (nullable = true)\n |-- WorkDate: date (nullable = true)\n |-- Location: string (nullable = true)\n |-- Mode : string (nullable = true)\n\nroot\n |-- EmployeeID: string (nullable = true)\n |-- Name: string (nullable = true)\n |-- Department: string (nullable = true)\n |-- Project: string (nullable = true)\n |-- WorkHours: integer (nullable = true)\n |-- WorkDate: date (nullable = true)\n |-- Location: string (nullable = true)\n |-- Mode: string (nullable = true)\n\n+----------+-----+----------+-------+---------+----------+---------+-------+\n|EmployeeID| Name|Department|Project|WorkHours|  WorkDate| Location|   Mode|\n+----------+-----+----------+-------+---------+----------+---------+-------+\n|      E101|Anita|        IT|  Alpha|        8|2024-05-01|Bangalore|Remote |\n|      E102|  Raj|        HR|   Beta|        7|2024-05-01|   Mumbai|Onsite |\n|      E103| John|   Finance|  Alpha|        5|2024-05-02|    Delhi|Remote |\n|      E101|Anita|        IT|  Alpha|        9|2024-05-03|Bangalore|Remote |\n|      E104|Meena|        IT|  Gamma|        6|2024-05-03|Hyderabad|Onsite |\n|      E102|  Raj|        HR|   Beta|        8|2024-05-04|   Mumbai| Remote|\n+----------+-----+----------+-------+---------+----------+---------+-------+\n\n+----------+-----+----------+-------+---------+----------+---------+-------+---------+\n|EmployeeID| Name|Department|Project|WorkHours|  WorkDate| Location|  Mode |  Weekday|\n+----------+-----+----------+-------+---------+----------+---------+-------+---------+\n|      E101|Anita|        IT|  Alpha|        8|2024-05-01|Bangalore|Remote |Wednesday|\n|      E102|  Raj|        HR|   Beta|        7|2024-05-01|   Mumbai|Onsite |Wednesday|\n|      E103| John|   Finance|  Alpha|        5|2024-05-02|    Delhi|Remote | Thursday|\n|      E101|Anita|        IT|  Alpha|        9|2024-05-03|Bangalore|Remote |   Friday|\n|      E104|Meena|        IT|  Gamma|        6|2024-05-03|Hyderabad|Onsite |   Friday|\n|      E102|  Raj|        HR|   Beta|        8|2024-05-04|   Mumbai| Remote| Saturday|\n+----------+-----+----------+-------+---------+----------+---------+-------+---------+\n\n+----------+-----------------+\n|Department|     AverageHours|\n+----------+-----------------+\n|        HR|              7.5|\n|   Finance|              5.0|\n|        IT|7.666666666666667|\n+----------+-----------------+\n\n+----------+----------+----+\n|EmployeeID|TotalHours|rank|\n+----------+----------+----+\n|      E101|        17|   1|\n|      E102|        15|   2|\n+----------+----------+----+\n\n+----------+----+----------+-------+---------+----------+--------+------+--------+\n|EmployeeID|Name|Department|Project|WorkHours|  WorkDate|Location| Mode | Weekday|\n+----------+----+----------+-------+---------+----------+--------+------+--------+\n|      E102| Raj|        HR|   Beta|        8|2024-05-04|  Mumbai|Remote|Saturday|\n+----------+----+----------+-------+---------+----------+--------+------+--------+\n\n+----------+----------+---------+-----------------+\n|EmployeeID|  WorkDate|WorkHours|RunningTotalHours|\n+----------+----------+---------+-----------------+\n|      E101|2024-05-01|        8|                8|\n|      E101|2024-05-03|        9|               17|\n|      E102|2024-05-01|        7|                7|\n|      E102|2024-05-04|        8|               15|\n|      E103|2024-05-02|        5|                5|\n|      E104|2024-05-03|        6|                6|\n+----------+----------+---------+-----------------+\n\n+----------+-----+----------+--------+-------+---------+\n|EmployeeID| Name|Department|DeptHead|Project|WorkHours|\n+----------+-----+----------+--------+-------+---------+\n|      E101|Anita|        IT|   Anand|  Alpha|        8|\n|      E102|  Raj|        HR|  Shruti|   Beta|        7|\n|      E103| John|   Finance|   Kamal|  Alpha|        5|\n|      E101|Anita|        IT|   Anand|  Alpha|        9|\n|      E104|Meena|        IT|   Anand|  Gamma|        6|\n|      E102|  Raj|        HR|  Shruti|   Beta|        8|\n+----------+-----+----------+--------+-------+---------+\n\n+----------+-----+----+-----+\n|EmployeeID|Alpha|Beta|Gamma|\n+----------+-----+----+-----+\n|      E103|    5|NULL| NULL|\n|      E104| NULL|NULL|    6|\n|      E101|   17|NULL| NULL|\n|      E102| NULL|  15| NULL|\n+----------+-----+----+-----+\n\n+----------+-----------+-----------+\n|EmployeeID|RemoteHours|OnsiteHours|\n+----------+-----------+-----------+\n|      E101|         12|          4|\n|      E102|          6|          9|\n|      E103|         10|          0|\n+----------+-----------+-----------+\n\n+----------+------+-----+\n|EmployeeID|  Mode|Hours|\n+----------+------+-----+\n|      E101|Remote|   12|\n|      E101|Onsite|    4|\n|      E102|Remote|    6|\n|      E102|Onsite|    9|\n|      E103|Remote|   10|\n|      E103|Onsite|    0|\n+----------+------+-----+\n\n+----------+---------+----------------+\n|EmployeeID|WorkHours|WorkloadCategory|\n+----------+---------+----------------+\n|      E101|        8|            Full|\n|      E102|        7|         Partial|\n|      E103|        5|         Partial|\n|      E101|        9|            Full|\n|      E104|        6|         Partial|\n|      E102|        8|            Full|\n+----------+---------+----------------+\n\n+----------+-----+----------+-------+---------+----------+---------+-------+---------+\n|EmployeeID| Name|Department|Project|WorkHours|  WorkDate| Location|  Mode |  Weekday|\n+----------+-----+----------+-------+---------+----------+---------+-------+---------+\n|      E101|Anita|        IT|  Alpha|        8|2024-05-01|Bangalore|Remote |Wednesday|\n|      E102|  Raj|        HR|   Beta|        7|2024-05-01|   Mumbai|Onsite |Wednesday|\n|      E103| John|   Finance|  Alpha|        5|2024-05-02|    Delhi|Remote | Thursday|\n|      E101|Anita|        IT|  Alpha|        9|2024-05-03|Bangalore|Remote |   Friday|\n|      E104|Meena|        IT|  Gamma|        6|2024-05-03|Hyderabad|Onsite |   Friday|\n|      E102|  Raj|        HR|   Beta|        8|2024-05-04|   Mumbai| Remote| Saturday|\n+----------+-----+----------+-------+---------+----------+---------+-------+---------+\n\n+----------+-------+\n|EmployeeID|   Mode|\n+----------+-------+\n|      E101|Remote |\n|      E102|   NULL|\n|      E103|Remote |\n|      E101|Remote |\n|      E104|Onsite |\n|      E102|   NULL|\n+----------+-------+\n\n+----------+------------+\n|EmployeeID|        Mode|\n+----------+------------+\n|      E101|     Remote |\n|      E102|Not Provided|\n|      E103|     Remote |\n|      E101|     Remote |\n|      E104|     Onsite |\n|      E102|Not Provided|\n+----------+------------+\n\n+----------+---------+\n|EmployeeID|WorkHours|\n+----------+---------+\n|      E101|        8|\n|      E102|        7|\n|      E103|        5|\n|      E101|        9|\n|      E104|        6|\n|      E102|        8|\n+----------+---------+\n\n+----------+--------------+------------+\n|EmployeeID|remote_percent|RemoteWorker|\n+----------+--------------+------------+\n|      E103|           0.0|          No|\n|      E104|           0.0|          No|\n|      E101|           0.0|          No|\n|      E102|          50.0|          No|\n+----------+--------------+------------+\n\n+----------+-----+----------+-------+---------+----------+---------+-------+---------+\n|EmployeeID| Name|Department|Project|WorkHours|  WorkDate| Location|   Mode|  Weekday|\n+----------+-----+----------+-------+---------+----------+---------+-------+---------+\n|      E101|Anita|        IT|  Alpha|        9|2024-05-03|Bangalore|Remote |   Friday|\n|      E103| John|   Finance|  Alpha|        5|2024-05-02|    Delhi|Remote | Thursday|\n|      E104|Meena|        IT|  Gamma|        6|2024-05-03|Hyderabad|Onsite |   Friday|\n|      E102|  Raj|        HR|   Beta|        7|2024-05-01|   Mumbai|Onsite |Wednesday|\n|      E101|Anita|        IT|  Alpha|        8|2024-05-01|Bangalore|Remote |Wednesday|\n|      E102|  Raj|        HR|   Beta|        8|2024-05-04|   Mumbai| Remote| Saturday|\n|      E201|  Ria|    Intern|  Delta|        6|2024-05-05|  Chennai| Remote|     NULL|\n|      E202|Arjun|    Intern|  Delta|        5|2024-05-05|  Kolkata| Onsite|     NULL|\n+----------+-----+----------+-------+---------+----------+---------+-------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "#  Task Set â€“ Intermediate to Advanced PySpark (No DLT)\n",
    "#  Data Ingestion & Schema Handling\n",
    "#  1. Load the CSV using inferred schema.\n",
    "from pyspark.sql.functions import *\n",
    "empdf=spark.read.csv(\"file:/Workspace/Shared/employee17.csv\",header=True,inferSchema=True)\n",
    "empdf.show()\n",
    "empdf.printSchema()\n",
    "#  2. Load the same file with schema explicitly defined.\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"EmployeeID\", StringType(), True),\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Department\", StringType(), True),\n",
    "    StructField(\"Project\", StringType(), True),\n",
    "    StructField(\"WorkHours\", IntegerType(), True),\n",
    "    StructField(\"WorkDate\", DateType(), True),\n",
    "    StructField(\"Location\", StringType(), True),\n",
    "    StructField(\"Mode\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_explicit = spark.read.option(\"header\", True).schema(schema).csv(\"file:/Workspace/Shared/employee17.csv\")\n",
    "df_explicit.printSchema()\n",
    "df_explicit.show()\n",
    "\n",
    "#  3. Add a new column Weekday extracted from WorkDate .\n",
    "empdf=empdf.withColumn(\"Weekday\",date_format(col(\"WorkDate\"),\"EEEE\"))\n",
    "empdf.show()\n",
    "#  Aggregations & Grouping\n",
    "#  4. Calculate total work hours by employee.\n",
    "tothrs=empdf.groupBy(\"EmployeeID\").agg(sum(\"WorkHours\").alias(\"TotalHours\"))\n",
    "#  5. Calculate average work hours per department.\n",
    "empdf.groupBy(\"Department\").agg(avg(\"WorkHours\").alias(\"AverageHours\")).show()\n",
    "#  6. Get top 2 employees by total hours using window function.\n",
    "from pyspark.sql.window import Window\n",
    "window_spec = Window.orderBy(tothrs[\"TotalHours\"].desc())\n",
    "top_2_employees = tothrs.withColumn(\"rank\", row_number().over(window_spec)).filter(\"rank <= 2\")\n",
    "top_2_employees.show()\n",
    "#  Date Operations\n",
    "#  7. Filter entries where  WorkDate falls on a weekend.\n",
    "empdf.filter((dayofweek(col(\"WorkDate\"))==1) | (dayofweek(col(\"WorkDate\"))==7)).show()\n",
    "#  8. Calculate running total of hours per employee using window.\n",
    "window_spec_emp= Window.partitionBy(\"EmployeeID\").orderBy(\"WorkDate\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "df_running_total = empdf.withColumn(\"RunningTotalHours\", sum(\"WorkHours\").over(window_spec_emp))\n",
    "df_running_total.select(\"EmployeeID\", \"WorkDate\", \"WorkHours\", \"RunningTotalHours\").show()\n",
    "#  Joining DataFrames\n",
    "#  9. Create \n",
    "# department_location.csv :\n",
    "#  Department,DeptHead \n",
    "# IT,Anand \n",
    "# HR,Shruti \n",
    "# Finance,Kamal\n",
    "deptloc= spark.createDataFrame([\n",
    "    (\"IT\", \"Anand\"),\n",
    "    (\"HR\", \"Shruti\"),\n",
    "    (\"Finance\", \"Kamal\"),\n",
    "], [\"Department\", \"DeptHead\"])\n",
    "#  10. Join with timesheet data and list all employees with their DeptHead.\n",
    "timesheet_with_head = empdf.join(deptloc, on=\"Department\", how=\"left\")\n",
    "timesheet_with_head.select(\"EmployeeID\", \"Name\", \"Department\", \"DeptHead\", \"Project\", \"WorkHours\").show()\n",
    "#  Pivot & Unpivot\n",
    "#  11. Pivot table: total hours per employee per project.\n",
    "pivot_df = empdf.groupBy(\"EmployeeID\").pivot(\"Project\").agg(sum(\"WorkHours\"))\n",
    "pivot_df.show()\n",
    "#  12. Unpivot example: Convert mode-specific hours into rows.\n",
    "data = [\n",
    "    (\"E101\", 12, 4),\n",
    "    (\"E102\", 6, 9),\n",
    "    (\"E103\", 10, 0),\n",
    "]\n",
    "\n",
    "columns = [\"EmployeeID\", \"RemoteHours\", \"OnsiteHours\"]\n",
    "\n",
    "wide_df = spark.createDataFrame(data, columns)\n",
    "wide_df.show()\n",
    "\n",
    "unpivoted = wide_df.selectExpr(\"EmployeeID\",\n",
    "    \"stack(2, 'Remote', RemoteHours, 'Onsite', OnsiteHours) as (Mode, Hours)\"\n",
    ")\n",
    "unpivoted.show()\n",
    "\n",
    "#  UDF & Conditional Logic\n",
    "#  13. Create a UDF to classify work hours:\n",
    "# def workload_tag(hours): \n",
    "# if hours >= 8: return \"Full\" \n",
    "# elif hours >= 4: return \"Partial\" \n",
    "# else: return \"Light\"\n",
    "#  14. Add a column \n",
    "# WorkloadCategory using this UDF.\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def workload_tag(hours):\n",
    "    if hours >= 8:\n",
    "        return \"Full\"\n",
    "    elif hours >= 4:\n",
    "        return \"Partial\"\n",
    "    else:\n",
    "        return \"Light\"\n",
    "\n",
    "workload_udf = udf(workload_tag, StringType())\n",
    "df_with_workload = empdf.withColumn(\"WorkloadCategory\", workload_udf(empdf[\"WorkHours\"]))\n",
    "df_with_workload.select(\"EmployeeID\", \"WorkHours\", \"WorkloadCategory\").show()\n",
    "\n",
    "#  Nulls and Cleanup\n",
    "#  15. Introduce some nulls in Mode column.\n",
    "# Introduce nulls: randomly nullify Mode for some employees\n",
    "empdf.show()\n",
    "empdf = empdf.withColumnRenamed(\"Mode \", \"Mode\")\n",
    "null_mode = empdf.withColumn(\"Mode\",when(col(\"EmployeeID\") == \"E102\", None).otherwise(col(\"Mode\")))\n",
    "null_mode.select(\"EmployeeID\", \"Mode\").show()\n",
    "#  16. Fill nulls with \"Not Provided\".\n",
    "filled_mode = null_mode.fillna({\"Mode\": \"Not Provided\"})\n",
    "filled_mode.select(\"EmployeeID\", \"Mode\").show()\n",
    "#  17. Drop rows where \n",
    "# WorkHours < 4.\n",
    "filtered_hours = filled_mode.filter(filled_mode[\"WorkHours\"] >= 4)\n",
    "filtered_hours.select(\"EmployeeID\", \"WorkHours\").show()\n",
    "#  Advanced Conditions\n",
    "#  18. Use \n",
    "# when-otherwise to mark employees as \"Remote Worker\" if >80% entries are\n",
    "#  Remote.\n",
    "from pyspark.sql.functions import col, count, when, expr\n",
    "\n",
    "# Step 1: Count total and remote entries per employee\n",
    "remote_ratio_df = empdf.groupBy(\"EmployeeID\").agg(\n",
    "    count(\"*\").alias(\"total_entries\"),\n",
    "    count(when(col(\"Mode\") == \"Remote\", True)).alias(\"remote_entries\")\n",
    ")\n",
    "\n",
    "remote_ratio_df = remote_ratio_df.withColumn(\n",
    "    \"remote_percent\",\n",
    "    (col(\"remote_entries\") / col(\"total_entries\")) * 100\n",
    ")\n",
    "\n",
    "remote_ratio_df = remote_ratio_df.withColumn(\n",
    "    \"RemoteWorker\",\n",
    "    when(col(\"remote_percent\") > 80, \"Yes\").otherwise(\"No\")\n",
    ")\n",
    "\n",
    "remote_ratio_df.select(\"EmployeeID\", \"remote_percent\", \"RemoteWorker\").show()\n",
    "\n",
    "#  19. Add a new column \n",
    "# ExtraHours where hours > 8.\n",
    "extra_hours_df = empdf.withColumn(\n",
    "    \"ExtraHours\",\n",
    "    when(col(\"WorkHours\") > 8, col(\"WorkHours\") - 8).otherwise(0)\n",
    ")\n",
    "#  Union + Duplicate Handling\n",
    "#  20. Append a dummy timesheet for new interns using unionByName() \n",
    "from pyspark.sql import Row\n",
    "\n",
    "intern_data = [\n",
    "    Row(EmployeeID=\"E201\", Name=\"Ria\", Department=\"Intern\", Project=\"Delta\", WorkHours=6,\n",
    "        WorkDate=\"2024-05-05\", Location=\"Chennai\", Mode =\"Remote\"),\n",
    "    Row(EmployeeID=\"E202\", Name=\"Arjun\", Department=\"Intern\", Project=\"Delta\", WorkHours=5,\n",
    "        WorkDate=\"2024-05-05\", Location=\"Kolkata\", Mode=\"Onsite\")\n",
    "]\n",
    "\n",
    "intern_df = spark.createDataFrame(intern_data)\n",
    "\n",
    "combined_df = empdf.unionByName(intern_df, allowMissingColumns=True)\n",
    "\n",
    "dedup_df = combined_df.dropDuplicates()\n",
    "\n",
    "dedup_df.show()\n",
    "\n",
    "# 21. Remove duplicate rows based on all columns.\n",
    "dedup_by_subset = combined_df.dropDuplicates([\"EmployeeID\", \"WorkDate\"])\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "june 17 assignment3",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}